{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Evaluation Project 6 Loan Application Status Prediction </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Problem Statement:</b>\n",
    "This dataset includes details of applicants who have applied for loan. The dataset includes details like credit history, loan amount, their income, dependents etc. \n",
    "\n",
    "#### <b>Independent Variables:</b>\n",
    "\n",
    "- Loan_ID\n",
    "\n",
    "- Gender\n",
    "\n",
    "- Married\n",
    "\n",
    "- Dependents\n",
    "\n",
    "- Education\n",
    "\n",
    "- Self_Employed\n",
    "\n",
    "- ApplicantIncome\n",
    "\n",
    "- CoapplicantIncome\n",
    "\n",
    "- Loan_Amount\n",
    "\n",
    "- Loan_Amount_Term\n",
    "\n",
    "- Credit History\n",
    "\n",
    "- Property_Area\n",
    "\n",
    "Dependent Variable (Target Variable):\n",
    "\n",
    "- Loan_Status\n",
    "\n",
    "<b>You have to build a model that can predict whether the loan of the applicant will be approved or not on the basis of the details provided in the dataset.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: The link of the dataset is below. \n",
    "\n",
    "Downlaod Files:</b>\n",
    "\n",
    "https://github.com/dsrscientist/DSData/blob/master/loan_prediction.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Importing require library for performing EDA, Data Wrangling and data cleaning</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data wrangling purpose\n",
    "import numpy as np # Basic computation library\n",
    "import seaborn as sns # For Visualization \n",
    "import matplotlib.pyplot as plt # ploting package\n",
    "%matplotlib inline\n",
    "import warnings # Filtering warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Loan Predication CSV dataset file using pandas\n",
    "df=pd.read_csv('loan_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No of Rows:',df.shape[0])\n",
    "print('No. of Columns:',df.shape[1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>  Before Going for Statistical exploration of data, first check integrity of data & Missing value </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Integrity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() # This check any if any duplicated entry exit in dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Since dataset is large,  Let check for any entry which is repeated or duplicated in dataset at same date. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "Dataset doesnot contain Any duplicate entry. So Yes To Go !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatype Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Comment </b>:\n",
    "    - In loan application status dataset we have 614 rows with 13 columns including target variable.\n",
    "    - A Target Variable is 'Loan_Status' having object datatype and It is categorical variable.\n",
    "    - Gender, Married, Education,Self Employed, Credit History, Loan Status are categorical features.\n",
    "    - There are three types of datatype dtypes: float64(4), int64(1), object(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Comment :</b>\n",
    "    - 7 out 13 columns contains missing value.\n",
    "    - As small amount of data is missing so we use mean amd mode to replace with NaN values.\n",
    "    \n",
    "<b> Lets explore categorical features before missing value imputation.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with Enlisting Value counts & Sub-categories of different categorial features available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category=['Gender','Married','Dependents','Education','Self_Employed',\n",
    "          'Loan_Amount_Term','Property_Area','Credit_History','Loan_Status']\n",
    "for i in category:\n",
    "    print(i)\n",
    "    print(df[i].value_counts())\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('gist_rainbow_r')\n",
    "plt.figure(figsize=(20,20),facecolor='white')\n",
    "plotnumber=1\n",
    "category=['Gender','Married','Dependents','Education','Self_Employed',\n",
    "          'Loan_Amount_Term','Property_Area','Credit_History','Loan_Status']\n",
    "for i in category:\n",
    "    if plotnumber<=9:\n",
    "        ax=plt.subplot(3,3,plotnumber)\n",
    "        sns.countplot(df[i])\n",
    "        plt.xlabel(i,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Comment:</b>\n",
    "     - Out of Total loan application 80 % applicants are Male. <b>We can Explore loan amount for each gender applied and evaluate whether on the same basis loan is approved for each gender or not?</b>\n",
    "     - Only 20% applicants are self employed. <b> So it will interesting to gain insight on relation between Applicant income and loan approval for non self employed category. We will look to find any benchmark range of Income for loan approval.Another benchmark we will try to find is about loan requirement for these two categories.</b>\n",
    "     - Nearly 70% are married and 75% of loan applicants are graduates\n",
    "     - Almost 60% of the applicants have no dependents.\n",
    "     - Most of applicants come from Semi Urban areas, followed by Urban and Rural areas.\n",
    "     - 80% people previously have credit history. Normally people having credit history are seen more prone to get loan approval.\n",
    "     - Nearly 70 % applicant gets loan approved.\n",
    "    \n",
    "<b> We can impute categoical variable with mode in that category. For numerical variable we have option of mean and median. If Outliers are to strech then we will impute with median.</b>\n",
    "    \n",
    "### Let check outliers for missing values Numerical variable having missing values by plotting boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot( y='LoanAmount', data=df,color='cyan')\n",
    "plt.ylabel('Loan Amount',fontsize=15)\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df['LoanAmount'], color='b')\n",
    "plt.xlabel('Loan Amount',fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of Loan Amount:\",df['LoanAmount'].mean())\n",
    "print(\"Median of Loan Amount:\",df['LoanAmount'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment -\n",
    "- The mean is greater than median loan amount.\n",
    "- Clearly we can see outliers in boxplot and feature is strecth to far in distribution plot.\n",
    "\n",
    "<b> As extreme outliers are present in feature and for that reason as data is more sensitive to mean we are going to impute missing values in <u>loan amount  with median.</u> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation details :\n",
    "1. Missing values in Loan amount is impute with median value.\n",
    "2. Maximum Loan term is 360 Months so Missing value in Loan amount term is replace with 360 Months.\n",
    "3. Credit History, Self Employed, dependents, Gender and Married are replace with mode of repective features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputating Missing value with mode for categorical features\n",
    "df['Credit_History'].fillna(df['Credit_History'].mode()[0],inplace=True)\n",
    "df['Self_Employed'].fillna(df['Self_Employed'].mode()[0],inplace=True)\n",
    "df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
    "df['Gender'].fillna(df['Gender'].mode()[0],inplace=True)\n",
    "df['Married'].fillna(df['Married'].mode()[0],inplace=True)\n",
    "\n",
    "# Imputation of Numerical features\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0],inplace=True)\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Check After Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "<b> Finally, No Missing Value is Present.\n",
    "\n",
    "We are Now Yes To Go Further !!!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the statistics of the columns using heatmap.\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.describe(),linewidths = 0.1,fmt='0.1f',annot = True,cmap='PiYG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- In Applicant Income & Coapplicant Income Std deviation value is greater than median. So data is spread and skewed.\n",
    "- Taking 75% and Max rows into consideration we can surely say that Outliers exist in Applicant Income, Coapplicant Income,Loan Amount.\n",
    "- Since Credit History is Categorical variable there is no significance in different statstical parameter of it.\n",
    "- Minimum Tenure for Loan is 12 Months and Maximum Loan tenure is 480 Months.\n",
    "- Minimum Applicant income is 150 and maximum is 81000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Let dive into exploration of Target and independent feature.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('husl')\n",
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "df['Loan_Status'].value_counts().plot.pie(explode=[0,0.1],autopct='%2.1f%%',\n",
    "                                          textprops ={ 'fontweight': 'bold','fontsize':13}, ax=ax[0],shadow=True)\n",
    "ax[0].set_title('Loan Status', fontsize=20,fontweight ='bold')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('Loan_Status',data=df,ax=ax[1])\n",
    "ax[1].set_title('Loan Status',fontsize=20,fontweight ='bold')\n",
    "ax[1].set_xlabel(\"Loan Status\",fontsize=18,fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- 68.7% Applicants gets loan Approval.\n",
    "- We can see that dataset is imbalanced in nature.\n",
    "\n",
    "\n",
    "<b> Let check each feature against Target variable to gain insight into data. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Vs Loan Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('husl')\n",
    "f,ax=plt.subplots(1,2,figsize=(16,8))\n",
    "df['Gender'].value_counts().plot.pie(explode=[0,0.1],autopct='%2.1f%%',\n",
    "                                          textprops ={ 'fontweight': 'bold','fontsize':13}, ax=ax[0],shadow=True)\n",
    "ax[0].set_title('Gender', fontsize=20,fontweight ='bold')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('Gender',hue=\"Loan_Status\",data=df,ax=ax[1])\n",
    "ax[1].set_title('Gender Vs Loan Status',fontsize=20,fontweight ='bold')\n",
    "ax[1].set_xlabel(\"Loan Status\",fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=14,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Gender'],df[\"Loan_Status\"], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- 81.8% Applicant are male.\n",
    "- Irrespective Gender 65 % Applicant from each gender gets loan approval.<b>It means that gender doesnot play any role loan approval. No Discrimation on name of Gender done.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Married Vs Loan Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('husl')\n",
    "f,ax=plt.subplots(1,2,figsize=(16,8))\n",
    "df['Married'].value_counts().plot.pie(explode=[0,0.1],autopct='%2.1f%%',\n",
    "                                          textprops ={ 'fontweight': 'bold','fontsize':13}, ax=ax[0],shadow=True)\n",
    "ax[0].set_title('Married', fontsize=20,fontweight ='bold')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('Married',hue=\"Loan_Status\",data=df,ax=ax[1])\n",
    "ax[1].set_title('Married Vs Loan Status',fontsize=20,fontweight ='bold')\n",
    "ax[1].set_xlabel(\"Married\",fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=14,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([df['Gender'],df['Married']],[df.Loan_Status],margins=True).style.background_gradient(cmap='gist_rainbow_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- 65.3 % loan applicants are married. We can see from Count plot that <u> Married people have more possibility of getting loan approval.</u>\n",
    "- <b> But Here comes Interesting observation from crosstab :</b>\n",
    "    - Unmarried Women are more chances of getting loan approval compare to married women.<b>We will try to find which factor actually play deciding role here like education, employeement or Income.</b>\n",
    "    - Married Men are more chances of loan approval.\n",
    "    \n",
    "### Let check how number dependents play here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('Set2')\n",
    "f,ax=plt.subplots(1,2,figsize=(16,8))\n",
    "df['Dependents'].value_counts().plot.pie(explode=[0,0.1,0.15,0.2],autopct='%2.1f%%',\n",
    "                                          textprops ={ 'fontweight': 'bold','fontsize':13}, ax=ax[0],shadow=True)\n",
    "ax[0].set_title('No. of Dependents', fontsize=20,fontweight ='bold')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('Dependents',hue=\"Loan_Status\",data=df,ax=ax[1])\n",
    "ax[1].set_title('No. of Dependents Vs Loan Status',fontsize=20,fontweight ='bold')\n",
    "ax[1].set_xlabel(\"Dependents\",fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=14,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([df['Dependents']],[df.Loan_Status,df['Gender']],margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([df['Dependents'],df['Gender']],[df.Loan_Status],margins=True).style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- 58.6% Applicants have no dependents on them.33% Applicants have either 1 or 2 dependents.<b> It will interesting check whether with increase in number dependents their is increase in requirement loan amount.</b>\n",
    "- Female having zero dependents have more chances to get loan approval compare to Females with dependents.\n",
    "- For male Maximum loan approval comes with 0 dependents followed by 2 dependents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education Vs Loan status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('prism')\n",
    "f,ax=plt.subplots(1,2,figsize=(16,8))\n",
    "df['Education'].value_counts().plot.pie(explode=[0,0.1],autopct='%2.1f%%',\n",
    "                                          textprops ={ 'fontweight': 'bold','fontsize':13}, ax=ax[0],shadow=True)\n",
    "ax[0].set_title('Education', fontsize=20,fontweight ='bold')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('Education',hue=\"Loan_Status\",data=df,ax=ax[1])\n",
    "ax[1].set_title('Education Vs Loan Status',fontsize=20,fontweight ='bold')\n",
    "ax[1].set_xlabel(\"Education\",fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=14,fontweight ='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    pd.crosstab([df['Education'],df['Gender']],[df.Loan_Status],margins=True).style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- Graduate applicants are more likely to get loan approval irrespective gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category=['Gender','Married','Education','Self_Employed','Property_Area','Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Label Encoder on categorical variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for i in Category:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Outliers Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numerical =['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8),facecolor='white')\n",
    "plotnumber=1\n",
    "\n",
    "for column in Numerical:\n",
    "    if plotnumber<=4:\n",
    "        ax=plt.subplot(2,2,plotnumber)\n",
    "        sns.boxplot(df[column],color='c')\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> From Boxplot we can see outliers exist dataset.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping unnecessary columns\n",
    "df.drop([\"Loan_ID\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dependents'] = df.Dependents.map({'0':0,'1':1,'2':2,'3+':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dependents'] =pd.to_numeric(df['Dependents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z = np.abs(zscore(df))\n",
    "threshold = 3\n",
    "df1 = df[(z<3).all(axis = 1)]\n",
    "\n",
    "print (\"Shape of the dataframe before removing outliers: \", df.shape)\n",
    "print (\"Shape of the dataframe after removing outliers: \", df1.shape)\n",
    "print (\"Percentage of data loss post outlier removal: \", (df.shape[0]-df1.shape[0])/df.shape[0]*100)\n",
    "\n",
    "df=df1.copy() # reassigning the changed dataframe name to our original dataframe name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Percentage Data Loss :'+\"\\033[0m\",((614-577)/614)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><em> We are losing 6.02 % of data and which Acceptable.</em></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Skewness of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,5),facecolor='white')\n",
    "plotnum=1\n",
    "for col in Numerical:\n",
    "    if plotnum<=4:\n",
    "        plt.subplot(1,4,plotnum)\n",
    "        sns.distplot(df[col],color='r')\n",
    "        plt.xlabel(col,fontsize=20)\n",
    "    plotnum+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- <b> Out of all above feature 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount' are skewed which are numerical feature.</b>\n",
    "- Other features are categorical in nature so skewness is nothing to do with these remaining feature.<u>We will ignore them.</u>\n",
    "- We will yeo-johnson transformation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing skewness using yeo-johnson  method to get better prediction\n",
    "skew = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "scaler = PowerTransformer(method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[skew] = scaler.fit_transform(df[skew].values)\n",
    "df[skew].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Checking skewness after using yeo-johnson ethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> For Numerical variable skewness is within permissible limit.\n",
    "\n",
    "So Yes To Go Forward !!!\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Corrleation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21,13))\n",
    "sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.3f', \n",
    "            annot_kws={'size':10}, cmap=\"gist_stern\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,6))\n",
    "df.corr()['Loan_Status'].drop(['Loan_Status']).sort_values(ascending=False).plot(kind='bar',color = 'purple')\n",
    "plt.xlabel('Features',fontsize=15)\n",
    "plt.ylabel('Income',fontsize=15)\n",
    "plt.title('Correlation of features with Target Variable Loan_Status',fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "<b> Most of feature are poorly or moderately correlated with target variable expect Credit History. </b>\n",
    "-  Maximum correlation of 0.561 exist between Credit History and Loan status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Checking Multicollinearity between features using variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif= pd.DataFrame()\n",
    "vif['VIF']= [variance_inflation_factor(df.values,i) for i in range(df.shape[1])]\n",
    "vif['Features']= df.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> All features VIF is within permissible limit of 10. \n",
    "\n",
    "So No Need to Worry About Multicollinearity.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Balanceing Imbalanced target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Loan_Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> As Target variable data is Imbalanced in nature we will need to balance target variable.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in target and dependent feature\n",
    "X = df.drop(['Loan_Status'], axis =1)\n",
    "Y = df['Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampleing using SMOTE Techniques\n",
    "oversample = SMOTE()\n",
    "X, Y = oversample.fit_resample(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><em> We have successfully resolved the class imbalanced problem and now all the categories have same data ensuring that the ML model does not get biased towards one category.</em></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=99, test_size=.3)\n",
    "print('Training feature matrix size:',X_train.shape)\n",
    "print('Training target vector size:',Y_train.shape)\n",
    "print('Test feature matrix size:',X_test.shape)\n",
    "print('Test target vector size:',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best Random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\n",
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(1,250):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X_scale,Y,test_size = 0.3, random_state=i)\n",
    "    log_reg=LogisticRegression()\n",
    "    log_reg.fit(X_train,Y_train)\n",
    "    y_pred=log_reg.predict(X_test)\n",
    "    acc=accuracy_score(Y_test,y_pred)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "print('Best accuracy is', maxAccu ,'on Random_state', maxRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=78, test_size=.3)\n",
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(X_train,Y_train)\n",
    "y_pred=log_reg.predict(X_test)\n",
    "print('\\033[1m'+'Logistics Regression Evaluation'+'\\033[0m')\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Accuracy Score of Logistics Regression :'+'\\033[0m', accuracy_score(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Confusion matrix of Logistics Regression :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'classification Report of Logistics Regression'+'\\033[0m \\n',classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Optimal value of n_neighbors for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(25):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsClassifier(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train,Y_train)  #fit the model\n",
    "    y_pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(Y_test,y_pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    print('RMSE value for k= ' , K , 'is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the rmse values against k values -\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(range(25), rmse_val, color='blue', linestyle='dashed', marker='o', markerfacecolor='green', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment- \n",
    "At k=3, we get the minimum RMSE value which approximately 0.44814821218396267, and shoots up on further increasing the k value. We can safely say that k=3 will give us the best result in this case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying other classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "model=[ \n",
    "        SVC(),\n",
    "        GaussianNB(),\n",
    "        DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors = 3),\n",
    "        RandomForestClassifier(),\n",
    "        ExtraTreesClassifier()]\n",
    "        \n",
    "for m in model:\n",
    "    m.fit(X_train,Y_train)\n",
    "    y_pred=m.predict(X_test)\n",
    "    print('\\033[1m'+'Classification ML Algorithm Evaluation Matrix',m,'is' +'\\033[0m')\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Confusion matrix :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Classification Report :'+'\\033[0m \\n',classification_report(Y_test, y_pred))\n",
    "    print('\\n')\n",
    "    disp = plot_roc_curve(m,X_test,Y_test)   \n",
    "    plt.legend(prop={'size':11}, loc='lower right')\n",
    "    plt.show()\n",
    "    print('============================================================================================================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossValidation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model=[LogisticRegression(),\n",
    "        SVC(),\n",
    "        GaussianNB(),\n",
    "        DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors = 3),\n",
    "        RandomForestClassifier(),\n",
    "        ExtraTreesClassifier()]\n",
    "\n",
    "for m in model:\n",
    "    score = cross_val_score(m, X_scale, Y, cv =5)\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Cross Validation Score', m, ':'+'\\033[0m\\n')\n",
    "    print(\"Score :\" ,score)\n",
    "    print(\"Mean Score :\",score.mean())\n",
    "    print(\"Std deviation :\",score.std())\n",
    "    print('\\n')\n",
    "    print('============================================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that  RandomForestClassifier() gives us good Accuracy and maximum f1 score along with best Cross-validation score.  we will apply Hyperparameter tuning on Random Forest model and Used it as final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning : GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {  'bootstrap': [True], 'max_depth': [5, 10,20,40,50,60], \n",
    "              'max_features': ['auto', 'log2'], \n",
    "              'criterion':['gini','entropy'],\n",
    "              'n_estimators': [5, 10, 15 ,25,50,60,70]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV = GridSearchCV(RandomForestClassifier(),parameter,verbose=10)\n",
    "GCV.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_mod = RandomForestClassifier(bootstrap=True,criterion='entropy',n_estimators= 50, max_depth=20 ,max_features='auto')\n",
    "Final_mod.fit(X_train,Y_train)\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "print('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "disp = plot_roc_curve(Final_mod,X_test,Y_test)   \n",
    "plt.legend(prop={'size':11}, loc='lower right')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.show()\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_score = roc_auc_score(Y_test, Final_mod.predict(X_test))\n",
    "print('\\033[1m'+'Auc Score :'+'\\033[0m\\n',auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(Final_mod,'Loan_Status_Final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
