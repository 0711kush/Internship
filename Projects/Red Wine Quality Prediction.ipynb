{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Wine Quality Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "\n",
    "The dataset is related to red and white variants of the Portuguese \"Vinho Verde\" wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
    "\n",
    "This dataset can be viewed as classification task. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute Information\n",
    "\n",
    "Input variables (based on physicochemical tests):\n",
    "\n",
    "1 - Fixed Acidity\n",
    "\n",
    "2 - Volatile Acidity\n",
    "\n",
    "3 - Citric Acid\n",
    "\n",
    "4 - Residual Sugar\n",
    "\n",
    "5 - Chlorides\n",
    "\n",
    "6 - Free Sulfur dioxide\n",
    "\n",
    "7 - Total Sulfur dioxide\n",
    "\n",
    "8 - Density\n",
    "\n",
    "9 - pH\n",
    "\n",
    "10 - Sulphates\n",
    "\n",
    "11 - Alcohol\n",
    "\n",
    "Output variable (based on sensory data):\n",
    "\n",
    "12 - Quality (score between 0 and 10)\n",
    "\n",
    "What might be an interesting thing to do, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good/1' and the remainder as 'not good/0'.\n",
    "This allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value.\n",
    "\n",
    "You need to build a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downlaod Files:\n",
    "https://github.com/dsrscientist/DSData/blob/master/winequality-red.csv\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('winequality-red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows, columns: \" + str(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata about features\n",
    "\n",
    "1. Alcohol: the amount of alcohol in wine\n",
    "2. Volatile acidity: are high acetic acid in wine which leads to an unpleasant vinegar taste\n",
    "3. Sulphates: a wine additive that contributes to SO2 levels and acts as an antimicrobial and antioxidant\n",
    "4. Citric Acid: acts as a preservative to increase acidity (small quantities add freshness and flavor to wines)\n",
    "5. Total Sulfur Dioxide: is the amount of free + bound forms of SO2\n",
    "6. Density: sweeter wines have a higher density\n",
    "7. Chlorides: the amount of salt in the wine\n",
    "8. Fixed acidity: are non-volatile acids that do not evaporate readily\n",
    "9. pH: the level of acidity\n",
    "10. Free Sulfur Dioxide: it prevents microbial growth and the oxidation of wine\n",
    "11. Residual sugar: is the amount of sugar remaining after fermentation stops. The key is to have a perfect balance between — sweetness and sourness (wines > 45g/ltrs are sweet)\n",
    "12. For the purpose of this project, I converted the output to a binary output where each wine is either “good quality” (a score of 7 or higher) or not (a score below 7).\n",
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives  :\n",
    "\n",
    "1. ML modelling with different classification algorithim to build model with highest accuracy which in turns lead to predicting quality of wine in term of good or not good.\n",
    "2. With help of EDA to determine which features are the most indicative of a good quality wine\n",
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment -\n",
    "1. In each feature we can see that mean is greater than median.\n",
    "2. Minimum value of citric acid is zero. Need check if it valid data or some kind of data error.\n",
    "3. There is lot difference between 75 th percentile and max in residual sugar, free sulfer dioxide, total sulfer dioxide.\n",
    "4. If we consider spread of data based on mean/std & right/left side skewed data based on 3rd quartile and max, we can definitely say that outliers are present in data.\n",
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean feature values as per different quality grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = pd.pivot_table(data=df, index='quality',aggfunc={'fixed acidity':np.mean, 'volatile acidity':np.mean, \n",
    "                                                     'citric acid':np.mean, \n",
    "                                                     'residual sugar':np.mean,'chlorides':np.mean,\n",
    "                                                     'free sulfur dioxide':np.mean,'density':np.mean,\n",
    "                                                     'pH':np.mean,'sulphates':np.mean,'alcohol':np.mean})\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment-\n",
    "#### Based on mean value quality\n",
    "1. Good quality (grade 7 & 8) of wine posses higher amount of alcohol, citric acid, fixed acidity, sulphates.\n",
    "2. Good quality (grade 7 & 8) of wine posses lower amount of Chlorides, low pH value,volatile acidity.\n",
    "3. Good quality (grade 7 & 8) of wine posses moderate amount of free sulfur dioxide in range of 14-16.\n",
    "4. Density and residual sugar are not deciding factor in determining quality of wine.\n",
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Create Classification version of target variable\n",
    "We will create two class for purpose of classification based on quality grade of red wine\n",
    "1. Class 1- Good quality red wine - if a quality grade of 7 or higher\n",
    "2. Class 2- Low quality red wine - if a quality grade less than 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] =[1 if x >= 7 else 0 for x in df['quality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.drop(columns='quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean feature values based on class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means = pd.pivot_table(data=df, index='class',aggfunc={'fixed acidity':np.mean, 'volatile acidity':np.mean, \n",
    "                                                     'citric acid':np.mean, \n",
    "                                                     'residual sugar':np.mean,'chlorides':np.mean,\n",
    "                                                     'free sulfur dioxide':np.mean,'density':np.mean,\n",
    "                                                     'pH':np.mean,'sulphates':np.mean,'alcohol':np.mean})\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = '1','0',\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(df['class'].value_counts(),labels = labels,radius =1,autopct = '%1.2f%%', shadow=True,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking null value or missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), cmap='hsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10, 7))\n",
    "sns.countplot(df['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "labels = '5','6','7','4','8','3'\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(df['quality'].value_counts(),labels = labels,radius =3 ,autopct = '%1.1f%%', shadow=True,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment -\n",
    "1. Majority of wine samples are of quality level 5 and 6.\n",
    "2. This dataset we have only 217 wine sample with higher quality grade.\n",
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in df:\n",
    "    if plotnumber <=12:\n",
    "        ax = plt.subplot(4,3,plotnumber)\n",
    "        sns.distplot(df[column], color='r')\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment - \n",
    "\n",
    "There is skewness in data\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in df:\n",
    "    if plotnumber <=12:\n",
    "        ax = plt.subplot(4,3,plotnumber)\n",
    "        plt.bar(df['quality'], df[column], color='b') \n",
    "        plt.xlabel('quality',fontsize=20)\n",
    "        plt.ylabel(column, fontsize =20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment-\n",
    "##### Based on quality\n",
    "1. Good quality (grade 7 & 8) of wine posses higher amount of alcohol, fixed acidity.\n",
    "2. Good quality (grade 7 & 8) of wine posses lower amount of low pH value,volatile acidity.\n",
    "3. Good quality (grade 7 & 8) of wine posses moderate amount of free sulfur dioxide in range of 14-16.\n",
    "4. Density and residual sugar are not deciding factor in determining quality of wine.\n",
    "5. Low grade quality of wine posses lower amount of total sulfer dioxide.\n",
    "6. Higher volatile acid lower the quality of wine.\n",
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,16), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in df:\n",
    "    if plotnumber <=12:\n",
    "        ax = plt.subplot(4,3,plotnumber)\n",
    "        sns.barplot(df['class'],df[column]) \n",
    "        plt.xlabel('class',fontsize=20)\n",
    "        plt.ylabel(column, fontsize =20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# class 1 - good quality\n",
    "# class 0 - low quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment - \n",
    "1. Quality of wine increase with increase in alcohol, sulpates, residual sugar, citric acid,fixed acidity.\n",
    "2. Quality of wine decreses with increase in total sulfur dioxide,chlorides, volatile acidity,free sulfur dioxide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,50), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in df:\n",
    "    if plotnumber <=12:\n",
    "        ax = plt.subplot(6,2,plotnumber)\n",
    "        sns.boxplot(df['quality'],df[column]) \n",
    "        plt.xlabel('quality',fontsize=20)\n",
    "        plt.ylabel(column, fontsize =20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grp_c=df.groupby('class')\n",
    "C_1=Grp_c.get_group(1)\n",
    "C_2=Grp_c.get_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber =1\n",
    "for column in C_1:\n",
    "    if plotnumber <=12:\n",
    "        ax = plt.subplot(4,3,plotnumber)\n",
    "        sns.kdeplot(C_1[column], color='b')\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df1, hue='class')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Detection basesd on IQR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =df1.copy()\n",
    "Q1 =df2.quantile(0.25)\n",
    "Q3= df2.quantile(0.75)\n",
    "IQR = Q3-Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new =df2[~((df <(Q1 - 1.5*IQR)) | (df >(Q3 + 1.5*IQR))).any(axis=1)]\n",
    "print(df_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Percentage Data Loss :'+\"\\033[0m\",((1599-1047)/1599)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is significant data loss  with IQR method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers using Z score Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "df3=df1.copy()\n",
    "z_score = zscore(df3)\n",
    "z_score_abs = np.abs(z_score)\n",
    "df_new= df3[(z_score_abs < 3).all(axis=1)]\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Percentage Data Loss :'+\"\\033[0m\",((1599-1458)/1599)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness detection and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data is highly skewed. So it need to transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming positive or right skew data using boxcox transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['fixed acidity']=boxcox(df_new['fixed acidity'],0)\n",
    "df_new['residual sugar']=boxcox(df_new['residual sugar'],-1)\n",
    "df_new['chlorides']=boxcox(df_new['chlorides'],-0.5)\n",
    "df_new['free sulfur dioxide']=boxcox(df_new['free sulfur dioxide'],0)\n",
    "df_new['total sulfur dioxide']=boxcox(df_new['total sulfur dioxide'],0)\n",
    "df_new['sulphates']=boxcox(df_new['sulphates'],0)\n",
    "df_new['alcohol']=boxcox(df_new['alcohol'],-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrleation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(12,10))\n",
    "sns.heatmap(df_new.corr(), annot= True ,cmap='Spectral')\n",
    "# cmap =PiYG cmap='Spectral'\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing correlation of feature columns with label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "df_new.corr()['class'].drop(['class']).plot(kind='bar',color = 'c')\n",
    "plt.xlabel('Features',fontsize=15)\n",
    "plt.ylabel('Class',fontsize=15)\n",
    "plt.title('Correlation of features with class',fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Multicollinearity between features using variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2=df_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif=pd.DataFrame()\n",
    "vif['vif'] = [variance_inflation_factor(df_new2.values,i) for i in range(df_new2.shape[1])]\n",
    "vif['Features']= df_new2.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pH, density are not contributing to label and also high multicollinearity exists. so we will drop density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2= df_new2.drop(['density','pH'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif=pd.DataFrame()\n",
    "vif['vif'] = [variance_inflation_factor(df_new2.values,i) for i in range(df_new2.shape[1])]\n",
    "vif['Features']= df_new2.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still there are lot of multicollinearity. So we need to scale data and apply pca dimensionilty reduction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_new.drop(columns=['class'])\n",
    "Y= df_new['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "#plot the graph to find the principal components\n",
    "x_pca = pca.fit_transform(X_scale)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment -\n",
    "###### AS per the graph, we can see that 8 principal components attribute for 90% of variation in the data.  We shall pick the first 8 components for our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_new = PCA(n_components=8)\n",
    "x_new = pca_new.fit_transform(X_scale)\n",
    "print(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x_new, Y, random_state=42, test_size=.3)\n",
    "print('Training feature matrix size:',X_train.shape)\n",
    "print('Training target vector size:',Y_train.shape)\n",
    "print('Test feature matrix size:',X_test.shape)\n",
    "print('Test target vector size:',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best Random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\n",
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(1,250):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(x_new,Y,test_size = 0.3, random_state=i)\n",
    "    log_reg=LogisticRegression()\n",
    "    log_reg.fit(X_train,Y_train)\n",
    "    y_pred=log_reg.predict(X_test)\n",
    "    acc=accuracy_score(Y_test,y_pred)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "print('Best accuracy is', maxAccu ,'on Random_state', maxRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x_new, Y, random_state=133, test_size=.33)\n",
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(X_train,Y_train)\n",
    "y_pred=log_reg.predict(X_test)\n",
    "print('\\033[1m'+'Logistics Regression Evaluation'+'\\033[0m')\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Accuracy Score of Logistics Regression :'+'\\033[0m', accuracy_score(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Confusion matrix of Logistics Regression :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'classification Report of Logistics Regression'+'\\033[0m \\n',classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Optimal value of n_neighbors for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(20):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsClassifier(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train,Y_train)  #fit the model\n",
    "    y_pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(Y_test,y_pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    print('RMSE value for k= ' , K , 'is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the rmse values against k values\n",
    "curve = pd.DataFrame(rmse_val) #elbow curve \n",
    "curve.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment- \n",
    "At k= 12, the RMSE is approximately 0.29868314022934306, and shoots up on further increasing the k value. We can safely say that k=12 will give us the best result in this case\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=[\n",
    "        SVC(),\n",
    "        GaussianNB(),\n",
    "        DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors = 12),\n",
    "        RandomForestClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "        BaggingClassifier()]\n",
    "\n",
    "for m in model:\n",
    "    m.fit(X_train,Y_train)\n",
    "    y_pred=m.predict(X_test)\n",
    "    print('\\033[1m'+'Classification ML Algorithm Evaluation Matrix',m,'is' +'\\033[0m')\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Confusion matrix :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Classification Report :'+'\\033[0m \\n',classification_report(Y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print('============================================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that  RandomForestClassifier() gives maximum Accuracy so we will continue further investigation with crossvalidation of above model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossValidation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model=[\n",
    "        SVC(),\n",
    "        GaussianNB(),\n",
    "        DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors = 12),\n",
    "        RandomForestClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "        BaggingClassifier()]\n",
    "\n",
    "for m in model:\n",
    "    score = cross_val_score(m, X, Y, cv =5)\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Cross Validation Score', m, ':'+'\\033[0m\\n')\n",
    "    print(\"Score :\" ,score)\n",
    "    print(\"Mean Score :\",score.mean())\n",
    "    print(\"Std deviation :\",score.std())\n",
    "    print('\\n')\n",
    "    print('============================================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that Random Forest Classifier gives maximum Accuracy. So we will apply Hyperparameter tuning on Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning : GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {'n_estimators':[30,50,60],'max_depth': [10,20,40,60,80],\n",
    "             'criterion':['gini','entropy'],'max_features':[\"auto\",\"sqrt\",\"log2\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV = GridSearchCV(RandomForestClassifier(),parameter,cv=5,n_jobs = -1)\n",
    "GCV.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_mod = RandomForestClassifier(criterion='entropy',n_estimators= 50, max_depth=20 ,max_features='sqrt')\n",
    "Final_mod.fit(X_train,Y_train)\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "print('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y_pred_prob = Final_mod.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(Y_test,y_pred_prob)\n",
    "plt.plot([0,1],[0,1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Random Forest Classifier')\n",
    "plt.xlabel('False postive rate')\n",
    "plt.ylabel('True postive rate')\n",
    "plt.show()\n",
    "auc_score = roc_auc_score(Y_test, Final_mod.predict(X_test))\n",
    "print('\\033[1m'+'Auc Score :'+'\\033[0m\\n',auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(Final_mod,'Red_Wine_Quality_Final.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
