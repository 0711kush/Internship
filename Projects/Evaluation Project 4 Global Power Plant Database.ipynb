{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Evaluation Project 4 Global Power Plant Database </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <b> Problem Statement:</b>\n",
    "<b><em> <u>Description</u>\n",
    "    \n",
    "The Global Power Plant Database is a comprehensive, open source database of power plants around the world. It centralizes power plant data to make it easier to navigate, compare and draw insights for oneâ€™s own analysis. The database covers approximately 35,000 power plants from 167 countries and includes thermal plants (e.g. coal, gas, oil, nuclear, biomass, waste, geothermal) and renewables (e.g. hydro, wind, solar). Each power plant is geolocated and entries contain information on plant capacity, generation, ownership, and fuel type. It will be continuously updated as data becomes available. </em></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key attributes of the database\n",
    "\n",
    "The database includes the following indicators:\n",
    "\n",
    "* `country` (text): 3 character country code corresponding to the ISO 3166-1 alpha-3 specification [5]\n",
    "* `country_long` (text): longer form of the country designation\n",
    "* `name` (text): name or title of the power plant, generally in Romanized form\n",
    "* `gppd_idnr` (text): 10 or 12 character identifier for the power plant\n",
    "* `capacity_mw` (number): electrical generating capacity in megawatts\n",
    "* `latitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "* `longitude` (number): geolocation in decimal degrees; WGS84 (EPSG:4326)\n",
    "* `primary_fuel` (text): energy source used in primary electricity generation or export\n",
    "* `other_fuel1` (text): energy source used in electricity generation or export\n",
    "* `other_fuel2` (text): energy source used in electricity generation or export\n",
    "* `other_fuel3` (text): energy source used in electricity generation or export\n",
    "* `commissioning_year` (number): year of plant operation, weighted by unit-capacity when data is available\n",
    "* `owner` (text): majority shareholder of the power plant, generally in Romanized form\n",
    "* `source` (text): entity reporting the data; could be an organization, report, or document, generally in Romanized form\n",
    "* `url` (text): web document corresponding to the `source` field\n",
    "* `geolocation_source` (text): attribution for geolocation information\n",
    "* `wepp_id` (text): a reference to a unique plant identifier in the widely-used PLATTS-WEPP database.\n",
    "* `year_of_capacity_data` (number): year the capacity information was reported\n",
    "* `generation_gwh_2013` (number): electricity generation in gigawatt-hours reported for the year 2013\n",
    "* `generation_gwh_2014` (number): electricity generation in gigawatt-hours reported for the year 2014\n",
    "* `generation_gwh_2015` (number): electricity generation in gigawatt-hours reported for the year 2015\n",
    "* `generation_gwh_2016` (number): electricity generation in gigawatt-hours reported for the year 2016\n",
    "* `generation_gwh_2017` (number): electricity generation in gigawatt-hours reported for the year 2017\n",
    "* `generation_gwh_2018` (number): electricity generation in gigawatt-hours reported for the year 2018\n",
    "* `generation_gwh_2019` (number): electricity generation in gigawatt-hours reported for the year 2019\n",
    "* `generation_data_source` (text): attribution for the reported generation information\n",
    "* `estimated_generation_gwh_2013` (number): estimated electricity generation in gigawatt-hours for the year 2013\n",
    "* `estimated_generation_gwh_2014` (number): estimated electricity generation in gigawatt-hours for the year 2014 \n",
    "* `estimated_generation_gwh_2015` (number): estimated electricity generation in gigawatt-hours for the year 2015 \n",
    "* `estimated_generation_gwh_2016` (number): estimated electricity generation in gigawatt-hours for the year 2016 \n",
    "* `estimated_generation_gwh_2017` (number): estimated electricity generation in gigawatt-hours for the year 2017 \n",
    "* `estimated_generation_note_2013` (text): label of the model/method used to estimate generation for the year 2013\n",
    "* `estimated_generation_note_2014` (text): label of the model/method used to estimate generation for the year 2014 \n",
    "* `estimated_generation_note_2015` (text): label of the model/method used to estimate generation for the year 2015\n",
    "* `estimated_generation_note_2016` (text): label of the model/method used to estimate generation for the year 2016\n",
    "* `estimated_generation_note_2017` (text): label of the model/method used to estimate generation for the year 2017 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuel Type Aggregation\n",
    "We define the \"Fuel Type\" attribute of our database based on common fuel categories. \n",
    "\n",
    "#### Prediction :   Make two prediction  1) Primary Fuel    2) capacity_mw \n",
    "\n",
    "<b>Find the dataset link below.\n",
    "\n",
    "Downlaod Files:</b>\n",
    "\n",
    "https://github.com/wri/global-power-plant-database/blob/master/source_databases_csv/database_IND.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Importing require library for performing EDA, Data Wrangling and data cleaning</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data wrangling purpose\n",
    "import numpy as np # Basic computation library\n",
    "import seaborn as sns # For Visualization \n",
    "import matplotlib.pyplot as plt # ploting package\n",
    "%matplotlib inline\n",
    "import warnings # Filtering warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing CSV File\n",
    "df=pd.read_csv('Powerplant_India.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No of Rows:',df.shape[0])\n",
    "print('No of Columns:',df.shape[1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- Dataset contain 908 rows with 25 columns.\n",
    "- Some of feature are with object datatypes and other with float.\n",
    "- other_fuel3,estimated_generation_gwh,wepp_id,other_fuel2 are empty columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>  Before Going for Statistical exploration of data, first check integrity of data & Missing value </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Integrity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Let check if any whitespace,'?' 'NA' or '-' exist in dataset. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isin([' ','NA','-','?']).sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- No whitespace, NA, '-' exist in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Since dataset is large, Let check for any entry which is repeated or duplicated in dataset. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() # This will check if any duplicate entry or duplicate row with same value exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><em> No Duplicate Entry Present in data.\n",
    "\n",
    "If we Check CSV file and look at dataset head, there are lot of data cleaning operation need to done before performing any EDA and ML modelling.At first sight we can come across following observation in CSV file:\n",
    "\n",
    "- Lot of missing data in certain columns.\n",
    "- Lot of  Non relevant data like gppd_idnr,url.\n",
    "- and many more.\n",
    "\n",
    "At end data need to clean and we will try to do some feature engineering afterwards to modify some columns.</em></b>\n",
    "### Start with looking at missing value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Before checking null value and missing value imputation , first remove empty columns and non relevalent columns.\n",
    "    \n",
    "Columns we are going remove are :\n",
    "    \n",
    "- estimated_generation_gwh - Empty\n",
    "- wepp_id - Empty\n",
    "- other_fuel2 - 98% data missing\n",
    "- other_fuel3 - Empty \n",
    "- owner - More than 60 % data missing\n",
    "- year_of_capacity_data - Missing data with single unique value\n",
    "- country - non relevalent info\n",
    "- country_long - non relevalent info\n",
    "- gppd_idnr -non relevalent info\n",
    "- url - No missing value but of no use\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"country\",\"country_long\",'url','year_of_capacity_data',\n",
    "                 'gppd_idnr','owner','other_fuel3','other_fuel2',\n",
    "                 'estimated_generation_gwh','wepp_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding what percentage of data is missing from the dataset\n",
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposed Strategy to Handle Missing data :\n",
    "\n",
    "- As Geolocation source is categorical data we can impute it with mode of category.\n",
    "- longitude and latitude can be impute with mean or median of longitude and latitude. This imputation will not distrub statstical balance of data as mean will be same at the end.\n",
    "- As commissioning year for most of industrial powerplant is missing(40%) after checking correlation we will decide to keep or drop this features.\n",
    "- In generation_data_source 50% data is missing and it doesnot have any importance in our analysis. So it is better to drop this feature.\n",
    "- There are 5 Different columns of GenerationGW-Hours for year 2013 to 2017. Its dive into it further\n",
    "    - It is important feature in for coal and hydro powerplant.\n",
    "    - It is natural to have missing data in this category. As Oil,Gas based Powerplant operated in intermitant periodic way and some renewable powerplant like wind,tide are operated seasonaly.\n",
    "    - Some new powerplant commission between 2013 and 2018. For these powerplant some data will definitely available.\n",
    "    - We cannot do any mean or median imputation here as different powerplant have different generation capacity & Generation per year depend on runtime of powerplant.\n",
    "    - We all know old powerplant normally kept off unless more demand of generation required. Reason to kept is low efficiency & high operating cost.\n",
    "    - We can neglect real value data for such important feature. We will keep this feature along with missing value and perform further investigation.\n",
    "- Other_fuel1 is another feature of some importance with missing value. Lets dive into it :\n",
    "    - Not every powerplant build to work with alternate fuel.\n",
    "    - Idea of other fuel is totally irrelevant to renewable energy source based powerplant like solar,wind,hydro.\n",
    "    - First talk about powerplant for which concept of other fuel is applicable. We can impute them based domain knowledge.\n",
    "        - Alternate Fuel of Coal based powerplant mostly is Oil or cogeneration.\n",
    "        - Alternate Fuel of Industrial Oil based powerplant is Gas.\n",
    "        - Alternate Fuel of Industrial Gas based powerplant is Oil.\n",
    "        - We cannot define any alternative fuel for nuclear powerplant as it sole based on plant design & so many option.\n",
    "    - For Renewable energy source based powerplant no alternate fuel needed. As it is categorical feature we can impute these powerplant with \"Not Applicable\". At end we are going Encoding these labels, 'Not Applicable' will be just one other additional label in encoding. Making no alternation on final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value Impuatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot( y='longitude', data=df,color='cyan')\n",
    "plt.ylabel('longitude',fontsize=15)\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df['longitude'], color='b')\n",
    "plt.xlabel('longitude',fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Based on presense of outliers we will impute longitude we with median. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot( y='latitude', data=df,color='cyan')\n",
    "plt.ylabel('latitude',fontsize=15)\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df['latitude'], color='b')\n",
    "plt.xlabel('latitude',fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Based on absense of outliers we will impute latitude we with mean. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value Imputation of geolocation with mode\n",
    "df['geolocation_source'] = df['geolocation_source'].fillna(df['geolocation_source'].mode()[0])\n",
    "# Missing Value Imputation of longitude with median\n",
    "df['longitude'] = df['longitude'].fillna(df['longitude'].median())\n",
    "# Missing Value Imputation of latitude with mean\n",
    "df['latitude'] = df['latitude'].fillna(df['latitude'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding what percentage of data is missing from the dataset\n",
    "missing_values = df.isnull().sum().sort_values(ascending = False)\n",
    "percentage_missing_values =(missing_values/len(df))*100\n",
    "print(pd.concat([missing_values, percentage_missing_values], axis =1, keys =['Missing Values', '% Missing data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statstical Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "- Bigest Powerplant has power generation capacity of 4760 MW.\n",
    "- In each Generation-GWHours columns Mean is greater than Median.\n",
    "- Powerplants are located in latitude range of 8.1689 to 34.6490 while longitudal range is 68.64 to 95.4080.\n",
    "- Oldest powerplant commission date back to 1927 and most recent powerplant is build in 2018. We will check in which timeframe most of powerplants are commission.\n",
    "- Median of capacity MW is 60 MW. This suggest that there are lot of small capacity powerplant in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><em> We have two different Target feature for regression and classification model. LetStart exploring both Target Feature</em> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Target feature - Capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Before visualing feature let find details about Smallest & Biggest Powerplant </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimum Capacity of Powerplant:',df.capacity_mw.min())\n",
    "print('Maximum Capacity of Powerplant:',df.capacity_mw.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.capacity_mw==4760]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.capacity_mw==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- <b> Biggest powerplant in India as per database is VINDH_CHAL STPS with Power generation capacity of 4760 MW.</b> As Name suggest it is Thermal powerplant with Coal as Primary Fuel and alternate fuel is oil. It is commission in year 2002.\n",
    "- Smallest power plant is mention as Tata BP - Electronics City Solar Panel Plant with capacity of 0 MW. <b>This must be data entry error as If powerplant is commission it cannot have capacity of 0 MW.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.violinplot(df.capacity_mw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We can see there are very powerplant of capacity more than 1500 MW. Let check</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.capacity_mw >1500].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> There only 15 powerplant of capacity more than 1500 MW.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['capacity_mw'],df[\"primary_fuel\"], margins=True).T.style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set_palette('husl')\n",
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "df['primary_fuel'].value_counts().plot.pie(autopct='%2.1f%%',\n",
    "                                          textprops ={ 'fontweight': 'bold','fontsize':13}, ax=ax[0],shadow=True)\n",
    "ax[0].set_title('primary_fuel', fontsize=20,fontweight ='bold')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('primary_fuel',data=df,ax=ax[1])\n",
    "ax[1].set_title('primary_fuel',fontsize=20,fontweight ='bold')\n",
    "ax[1].set_xlabel(\"primary_fuel\",fontsize=18,fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.primary_fuel.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- Maximum Number of powerplant are coal based powerplant followed by Hydro energy based powerplants.\n",
    "- <b>Around 40 % Power plant are Unconventional powerplant mainly based on Non Renewable energy resorces.</b>\n",
    "- There are only 9 Nuclear powerplant. Usually these powerplant build for high capacity.<b> It will be interesting to look for capacity of these powerplant.</b>\n",
    "\n",
    "<b> Let find Total Generation capacity of all powerplant, afterwards we will look for capacity of powerplant according to fuel type.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Power Generation Capacity of all Power Plants:',df['capacity_mw'].sum(),'MW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> India's Total Power Generation Capacity of is 291510.11 MW.\n",
    "\n",
    "Now Let check sector wise Capacity.\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"capacity_mw\"].groupby(df[\"primary_fuel\"]).agg([sum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "<b> Out of Total PowerGeneration Maximum comes from Coal. \n",
    "\n",
    "Here comes another interesting observation which we already suspected. Overall Power Generation capacity of Nuclear powerplants is greater than Unconventional Energy sources based powerplant (biomass, wind,solar) combined.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the relation between primary_fuel and  Average capacity_mw\n",
    "plt.figure(figsize = (10,8))\n",
    "y = df[\"capacity_mw\"]\n",
    "p = sns.barplot(x = \"primary_fuel\", y = \"capacity_mw\", data = df)\n",
    "plt.title('Comparision between Primary fuel Type and Mean Capacity in mw', fontsize=22, fontweight='bold')\n",
    "p.set_xlabel('Primary fuel Type',fontsize=18,fontweight ='bold')\n",
    "p.set_ylabel('Mean Capacity in mw',fontsize=18,fontweight ='bold')\n",
    "plt.xticks(fontsize=16,fontweight ='bold',rotation=30)\n",
    "plt.yticks(fontsize=16,fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Above result is obvious as only 9 Nuclear powerplant so its mean Capacity is higher.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(2,2,figsize=(15,12))\n",
    "\n",
    "# Checking generation growth in 2013\n",
    "sns.barplot(x='primary_fuel',y='generation_gwh_2013',ax=axes[0,0],data=df)\n",
    "\n",
    "# Checking generation growth in 2014\n",
    "sns.barplot(x='primary_fuel',y='generation_gwh_2014',ax=axes[0,1],data=df)\n",
    "\n",
    "# Checking generation growth in 2015\n",
    "sns.barplot(x='primary_fuel',y='generation_gwh_2015',ax=axes[1,0],data=df,palette=\"Set2\")\n",
    "\n",
    "# Checking generation growth in 2016\n",
    "sns.barplot(x='primary_fuel',y='generation_gwh_2016',ax=axes[1,1],data=df,palette=\"ch:.25\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- From the graph we can see that Nuclear has high mean electrical generation reports for all the years followed by Coal.\n",
    "Reason is high efficieny of Nuclear powerplants.\n",
    "- Other fuel types have very less contribution to power generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location vs Primary Fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how the primary_fuel is reated to longitude of the power plant\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.style.use('ggplot')\n",
    "plt.title('Comparision between longitude and primary_fuel')\n",
    "a = sns.boxplot(df['primary_fuel'],df[\"longitude\"])\n",
    "a.set_xlabel('Primary fuel Type',fontsize=18,fontweight ='bold')\n",
    "a.set_ylabel('longitude',fontsize=18,fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how the primary_fuel is reated to Latitude of the power plant\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.style.use('ggplot')\n",
    "plt.title('Comparision between longitude and primary_fuel')\n",
    "a = sns.boxplot(df['primary_fuel'],df[\"latitude\"])\n",
    "a.set_xlabel('Primary fuel Type',fontsize=18,fontweight ='bold')\n",
    "a.set_ylabel('latitude',fontsize=18,fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- Location is important feature specially for Nuclear powerplants. These plants lies in between 15 to 25 latitude and longitudnal range of 75 to 88 degree.\n",
    "- We can also see  solar power plant are mostly establish in certain range of latitude and longitude. This might be due to clear and intense sunlight with minimum humidity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location vs Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how the capacity is reated to latitude of the power plant\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.style.use('ggplot')\n",
    "plt.title('Comparision between latitude and capacity_mw',fontsize=22, fontweight='bold')\n",
    "a= sns.scatterplot(df['latitude'],df[\"capacity_mw\"])\n",
    "a.set_xlabel('Capacity',fontsize=18,fontweight ='bold')\n",
    "a.set_ylabel('latitude',fontsize=18,fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how the longitude related to the capacity of the power plant\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.style.use('ggplot')\n",
    "plt.title('Comparision between longitude and capacity_mw',fontsize=22, fontweight='bold')\n",
    "a= sns.regplot(df['longitude'],df[\"capacity_mw\"])\n",
    "a.set_xlabel('Primary fuel Type',fontsize=18,fontweight ='bold')\n",
    "a.set_ylabel('longitude',fontsize=18,fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Nothing significant insight gain from above plots</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commission age vs primary fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets extract power plant age from commissioning year by subtracting it from the year 2018\n",
    "df['Power_plant_age'] = 2018 - df['commissioning_year']\n",
    "df.drop(columns=['commissioning_year'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The oldest powerplant Age :',df.Power_plant_age.max())\n",
    "print('The youngest powerplant Age :',df.Power_plant_age.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Power_plant_age==91]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Oldest Powerplant is SIVASAMUNDRUM which is hydro power based plant with generation capacity of 42 MW.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check how the power plant age affects Fuel Type\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.title(\"Comparision between primary_fuel and Power plant age\", fontsize=22, fontweight='bold')\n",
    "a= sns.barplot(x = \"primary_fuel\", y = \"Power_plant_age\", data = df,palette=\"Set2\")\n",
    "a.set_xlabel('Primary fuel Type',fontsize=18,fontweight ='bold')\n",
    "a.set_ylabel('Power_plant_age',fontsize=18,fontweight ='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how the Power_plant_age affects the capacity of the power plant\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.title('Comparision between Power_plant_age and capacity_mw')\n",
    "sns.regplot(df['Power_plant_age'],df['capacity_mw'],color = \"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is negative linear relationship between feature and label.\n",
    "- We can say that the capacity of older plants is very less than compared to the powerplants which are stated recently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the relation between source and capacity_mw\n",
    "plt.figure(figsize = (10,6))\n",
    "sns.barplot(x = \"geolocation_source\", y = \"capacity_mw\", data = df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels='WRI','Industry About','National Renewable Energy Laboratory'\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.pie(df['geolocation_source'].value_counts(), labels=labels, autopct='%1.2f%%', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The capacity of geological source WRI is maximum which has capacity more than 350 megawatts. The geological source Industry About has less capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numerical=['capacity_mw', 'latitude', 'longitude', 'generation_gwh_2013', 'generation_gwh_2014', 'generation_gwh_2015',\n",
    "           'generation_gwh_2016', 'generation_gwh_2017', 'Power_plant_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how the data has been distributed in each column\n",
    "\n",
    "plt.figure(figsize=(20,15),facecolor='white')\n",
    "plotnumber=1\n",
    "for col in Numerical:\n",
    "    if plotnumber<=9:\n",
    "        ax=plt.subplot(3,3,plotnumber)\n",
    "        sns.distplot(df[col],color='b')\n",
    "        plt.xlabel(col,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- From the above distributionplot we can observe that the data is not normally distributed in all the columns except latitude and longitude.\n",
    "- In most of the columns the mean is greater than the median which means they are skewed to right.\n",
    "- We will remove the skewness in all these columns except the label later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(2,2,figsize=(15,12))\n",
    "\n",
    "# Checking generation growth in 2014\n",
    "sns.scatterplot(x='generation_gwh_2014',y='capacity_mw',ax=axes[0,0],data=df,color=\"g\")\n",
    "\n",
    "# Checking generation growth in 2015\n",
    "sns.scatterplot(x='generation_gwh_2015',y='capacity_mw',ax=axes[0,1],data=df,color=\"indigo\")\n",
    "\n",
    "# Checking generation growth in 2016\n",
    "sns.scatterplot(x='generation_gwh_2016',y='capacity_mw',ax=axes[1,0],data=df,color=\"g\")\n",
    "\n",
    "# Checking generation growth in 2017\n",
    "sns.scatterplot(x='generation_gwh_2017',y='capacity_mw',ax=axes[1,1],data=df,color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "- There is a positive linear relationship between the capacity and the electricity generation reported for the years mentioned.\n",
    "- The plot shows the electricity generation reported for the years 2014,2015,2016,2017 have high capacity of above 1000mw. Also the power generation growth is more than 5000gwh in all the years.\n",
    "- As the electricity generation growth increases, the capacity of plant also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the pairwise relation between the features and label capacity in megawatt.\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Comment:\n",
    "- Most of the features have strong linear relationship and correlation with each other.\n",
    "- From the plot we can observe the outliers in some of the columns.\n",
    "- We can see histogram on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['name'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['generation_data_source'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category=['primary_fuel', 'other_fuel1', 'source', 'geolocation_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Label Encoder on categorical variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for i in Category:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers detection and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the outliers present in numerical columns using boxplot\n",
    "\n",
    "plt.figure(figsize=(20,25))\n",
    "plotnumber=1\n",
    "for col in Numerical:\n",
    "    if plotnumber<=9:\n",
    "        ax=plt.subplot(3,3,plotnumber)\n",
    "        sns.boxplot(df[col],color='cyan')\n",
    "        plt.xlabel(col,fontsize=12)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment :\n",
    "- There outliers exist in data.\n",
    "- There are no outliers in lower capping level.\n",
    "- So we will use Quantile-based Flooring and Capping technique.\n",
    "\n",
    "### Further outliers investigation will be done bases Quantile-based Flooring and Capping technique. In this technique, we will do the flooring (e.g., the 10th percentile) for the lower values and capping (e.g., the 75th percentile) for the higher values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of unnessary columns\n",
    "df.drop(columns=['source','geolocation_source'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df.copy()\n",
    "Q1 = df3.quantile(0)\n",
    "Q3= df3.quantile(0.85)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df3[~((df3 < (Q1 - 1.5 * IQR)) |(df3 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+'Percentage Data Loss :'+\"\\033[0m\",((908-876)/876)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21,13))\n",
    "sns.heatmap(data.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.3f', \n",
    "            annot_kws={'size':10}, cmap=\"cool\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The label capacity_mw is highly positively  correlated with the features generation_gwh_2017,generation_gwh_2016,generation_gwh_2015,generation_gwh_2014,generation_gwh_2013.\n",
    "- capaity is negatively correlated with the features primary_fuel, source and Powe_plant_age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "data.corr()['capacity_mw'].sort_values(ascending=False).drop(['capacity_mw']).plot(kind='bar',color='c')\n",
    "plt.xlabel('Features',fontsize=10)\n",
    "plt.ylabel('Capacity',fontsize=10)\n",
    "plt.title('Correlation between capacity and features using bar plot',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "data.corr()['primary_fuel'].sort_values(ascending=False).drop(['capacity_mw']).plot(kind='bar',color='c')\n",
    "plt.xlabel('Features',fontsize=10)\n",
    "plt.ylabel('primary_fuel',fontsize=10)\n",
    "plt.title('Correlation between primary_fuel and features using bar plot',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = ['longitude','other_fuel1','generation_gwh_2013','generation_gwh_2014','generation_gwh_2015','generation_gwh_2016','generation_gwh_2017','Power_plant_age']\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "scaler = PowerTransformer(method = 'yeo-johnson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[skew] = scaler.fit_transform(data[skew].values)\n",
    "data[skew].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking skewness after using yeo-johnson\n",
    "data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop other fuel as result of poor correlation and lot of missing values\n",
    "data.drop(\"other_fuel1\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop other fuel as result of poor correlation and lot of missing values\n",
    "data.drop(\"other_fuel1\",axis=1,inplace=True)\n",
    "# Missing Value Imputation of latitude with mean\n",
    "data['Power_plant_age'] = data['Power_plant_age'].fillna(data['Power_plant_age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we have done with EDA NOW its time for removal of missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"generation_gwh_2013\",axis=1,inplace=True)\n",
    "data.drop(\"generation_gwh_2014\",axis=1,inplace=True)\n",
    "data.drop(\"generation_gwh_2015\",axis=1,inplace=True)\n",
    "data.drop(\"generation_gwh_2016\",axis=1,inplace=True)\n",
    "data.drop(\"generation_gwh_2017\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Building Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in target and dependent feature\n",
    "X = data.drop(['primary_fuel'], axis =1)\n",
    "Y = data['primary_fuel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=99, test_size=.3)\n",
    "print('Training feature matrix size:',X_train.shape)\n",
    "print('Training target vector size:',Y_train.shape)\n",
    "print('Test feature matrix size:',X_test.shape)\n",
    "print('Test target vector size:',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best Random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\n",
    "maxAccu=0\n",
    "maxRS=0\n",
    "for i in range(1,1000):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X_scale,Y,test_size = 0.3, random_state=i)\n",
    "    log_reg=LogisticRegression()\n",
    "    log_reg.fit(X_train,Y_train)\n",
    "    y_pred=log_reg.predict(X_test)\n",
    "    acc=accuracy_score(Y_test,y_pred)\n",
    "    if acc>maxAccu:\n",
    "        maxAccu=acc\n",
    "        maxRS=i\n",
    "print('Best accuracy is', maxAccu ,'on Random_state', maxRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=737, test_size=.3)\n",
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(X_train,Y_train)\n",
    "y_pred=log_reg.predict(X_test)\n",
    "print('\\033[1m'+'Logistics Regression Evaluation'+'\\033[0m')\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Accuracy Score of Logistics Regression :'+'\\033[0m', accuracy_score(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Confusion matrix of Logistics Regression :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'classification Report of Logistics Regression'+'\\033[0m \\n',classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=737, test_size=.3)\n",
    "dtc=DecisionTreeClassifier()\n",
    "dtc.fit(X_train,Y_train)\n",
    "y_pred=dtc.predict(X_test)\n",
    "print('\\033[1m'+'DecisionTreeClassifier Evaluation'+'\\033[0m')\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Accuracy Score of DecisionTreeClassifier :'+'\\033[0m', accuracy_score(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Confusion matrix of DecisionTreeClassifier :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'classification Report of DecisionTreeClassifier'+'\\033[0m \\n',classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=737, test_size=.3)\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.fit(X_train,Y_train)\n",
    "y_pred=rfc.predict(X_test)\n",
    "print('\\033[1m'+'RandomForestClassifier Evaluation'+'\\033[0m')\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Accuracy Score of RandomForestClassifier :'+'\\033[0m', accuracy_score(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Confusion matrix of RandomForestClassifier :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'classification Report of RandomForestClassifier'+'\\033[0m \\n',classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=737, test_size=.3)\n",
    "etc=ExtraTreesClassifier()\n",
    "etc.fit(X_train,Y_train)\n",
    "y_pred=etc.predict(X_test)\n",
    "print('\\033[1m'+'ExtraTreesClassifier Evaluation'+'\\033[0m')\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Accuracy Score of ExtraTreesClassifier :'+'\\033[0m', accuracy_score(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Confusion matrix of ExtraTreesClassifier :'+'\\033[0m \\n',confusion_matrix(Y_test, y_pred))\n",
    "print('\\n')\n",
    "print('\\033[1m'+'classification Report of ExtraTreesClassifier'+'\\033[0m \\n',classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model=[LogisticRegression(),\n",
    "       DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        ExtraTreesClassifier()]\n",
    "\n",
    "for m in model:\n",
    "    score = cross_val_score(m, X_scale, Y, cv =5)\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Cross Validation Score', m, ':'+'\\033[0m\\n')\n",
    "    print(\"Score :\" ,score)\n",
    "    print(\"Mean Score :\",score.mean())\n",
    "    print(\"Std deviation :\",score.std())\n",
    "    print('\\n')\n",
    "    print('============================================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that  RandomForestClassifier() gives us good Accuracy and maximum f1 score along with best Cross-validation score.  we will apply Hyperparameter tuning on Random Forest model and Used it as final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning : GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {  'bootstrap': [True], 'max_depth': [5, 10,20,40,50,60], \n",
    "              'max_features': ['auto', 'log2'], \n",
    "              'criterion':['gini','entropy'],\n",
    "              'n_estimators': [5, 10, 15 ,25,50,60,70]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV = GridSearchCV(RandomForestClassifier(),parameter,verbose=5)\n",
    "GCV.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_mod = RandomForestClassifier(bootstrap=True,criterion='gini',n_estimators=60, max_depth=50 ,max_features='auto')\n",
    "Final_mod.fit(X_train,Y_train)\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "print('\\033[1m'+'Accuracy Score :'+'\\033[0m\\n', accuracy_score(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = Final_mod.predict_proba(X_test)\n",
    "\n",
    "macro_roc_auc_ovo = roc_auc_score(Y_test, y_prob, multi_class=\"ovo\", average=\"macro\")\n",
    "weighted_roc_auc_ovo = roc_auc_score(Y_test, y_prob, multi_class=\"ovo\", average=\"weighted\")\n",
    "macro_roc_auc_ovr = roc_auc_score(Y_test, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
    "weighted_roc_auc_ovr = roc_auc_score(Y_test, y_prob, multi_class=\"ovr\", average=\"weighted\")\n",
    "print(\"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "      \"(weighted by prevalence)\"\n",
    "      .format(macro_roc_auc_ovo, weighted_roc_auc_ovo))\n",
    "print(\"=\"*40)\n",
    "print(\"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "      \"(weighted by prevalence)\"\n",
    "      .format(macro_roc_auc_ovr, weighted_roc_auc_ovr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as sktplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "class_names = df.columns\n",
    "metrics.plot_confusion_matrix(Final_mod, X_test, Y_test, cmap='mako')\n",
    "plt.title('\\t Confusion Matrix for Extra Trees Classifier \\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(Final_mod,'powerplant_classification_Final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Building Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import  GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import  Ridge\n",
    "from sklearn.linear_model import  Lasso\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2.copy()\n",
    "Q1 = df3.quantile(0)\n",
    "Q3= df3.quantile(0.85)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df3[~((df3 < (Q1 - 1.5 * IQR)) |(df3 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = ['longitude','other_fuel1','generation_gwh_2013','generation_gwh_2014','generation_gwh_2015','generation_gwh_2016','generation_gwh_2017','Power_plant_age']\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "scaler = PowerTransformer(method = 'yeo-johnson')\n",
    "data[skew] = scaler.fit_transform(data[skew].values)\n",
    "data[skew].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop other fuel as result of poor correlation and lot of missing values\n",
    "data.drop(\"other_fuel1\",axis=1,inplace=True)\n",
    "# Missing Value Imputation of latitude with mean\n",
    "data['Power_plant_age'] = data['Power_plant_age'].fillna(data['Power_plant_age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"generation_gwh_2013\",axis=1,inplace=True)\n",
    "data.drop(\"generation_gwh_2014\",axis=1,inplace=True)\n",
    "data.drop(\"generation_gwh_2015\",axis=1,inplace=True)\n",
    "data.drop(\"generation_gwh_2016\",axis=1,inplace=True)\n",
    "data.drop(\"generation_gwh_2017\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in target and dependent feature\n",
    "X = data.drop(['capacity_mw'], axis =1)\n",
    "Y = data['capacity_mw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=557, test_size=.33)\n",
    "print('Training feature matrix size:',X_train.shape)\n",
    "print('Training target vector size:',Y_train.shape)\n",
    "print('Test feature matrix size:',X_test.shape)\n",
    "print('Test target vector size:',Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "maxR2_score=0\n",
    "maxRS=0\n",
    "for i in range(1,1000):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=i, test_size=.33)\n",
    "    lin_reg=LinearRegression()\n",
    "    lin_reg.fit(X_train,Y_train)\n",
    "    y_pred=lin_reg.predict(X_test)\n",
    "    R2=r2_score(Y_test,y_pred)\n",
    "    if R2>maxR2_score:\n",
    "        maxR2_score=R2\n",
    "        maxRS=i\n",
    "print('Best R2 Score is', maxR2_score ,'on Random_state', maxRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, random_state=91, test_size=.3)\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train,Y_train)\n",
    "lin_reg.score(X_train,Y_train)\n",
    "y_pred=lin_reg.predict(X_test)\n",
    "print('\\033[1m'+'Predicted Wins:'+'\\033[0m\\n',y_pred)\n",
    "print('\\n')\n",
    "print('\\033[1m'+'Actual Wins:'+'\\033[0m\\n',Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "print('\\033[1m'+' Error :'+'\\033[0m')\n",
    "print('Mean absolute error :', mean_absolute_error(Y_test,y_pred))\n",
    "print('Mean squared error :', mean_squared_error(Y_test,y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(Y_test,y_pred)))\n",
    "print('\\n')\n",
    "from sklearn.metrics import r2_score\n",
    "print('\\033[1m'+' R2 Score :'+'\\033[0m')\n",
    "print(r2_score(Y_test,y_pred,multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying other ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "dtc = DecisionTreeRegressor()\n",
    "adb=AdaBoostRegressor(learning_rate=0.1)\n",
    "gradb=GradientBoostingRegressor()\n",
    "rd=Ridge(alpha=0.01)\n",
    "xgb=XGBRegressor()\n",
    "model = [rf,rd,dtc,adb,gradb,xgb]\n",
    "\n",
    "for m in model:\n",
    "    m.fit(X_train,Y_train)\n",
    "    m.score(X_train,Y_train)\n",
    "    y_pred = m.predict(X_test)\n",
    "    print('\\n')                                        \n",
    "    print('\\033[1m'+' Error of ', m, ':' +'\\033[0m')\n",
    "    print('Mean absolute error :', mean_absolute_error(Y_test,y_pred))\n",
    "    print('Mean squared error :', mean_squared_error(Y_test,y_pred))\n",
    "    print('Root Mean Squared Error:', np.sqrt(mean_squared_error(Y_test,y_pred)))\n",
    "    print('\\n')\n",
    "\n",
    "    print('\\033[1m'+' R2 Score :'+'\\033[0m')\n",
    "    print(r2_score(Y_test,y_pred)) \n",
    "    print('==============================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "model = [rf,rd,dtc,adb,gradb,xgb]\n",
    "\n",
    "for m in model:\n",
    "    score = cross_val_score(m, X_scale, Y, cv =5)\n",
    "    print('\\n')\n",
    "    print('\\033[1m'+'Cross Validation Score :',m,\":\"+'\\033[0m\\n')\n",
    "    print(\"Mean CV Score :\",score.mean())\n",
    "    print('==============================================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Among all Model Random Forest Regressor gave us maximum R2 score  and minimum  RMSE value  . So We will perform Hyper Parameter Tuning on Random Forest Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {'n_estimators':[50,60,80],'max_depth': [10,20,40],\n",
    "            'criterion':['mse','mae'],'min_samples_leaf': [5,10,15],\n",
    "             'min_samples_split':[5,10,15,20],\n",
    "             'max_features':[\"auto\",\"sqrt\",\"log2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV = GridSearchCV(RandomForestRegressor(),parameter,verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "Final_mod =  RandomForestRegressor(n_estimators=80 ,criterion = 'mse', max_depth= 40, max_features = 'sqrt',\n",
    "             min_samples_leaf = 5, min_samples_split = 10)\n",
    "Final_mod.fit(X_train,Y_train)\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "print('\\n')                                        \n",
    "print('\\033[1m'+' Error in Final Model :' +'\\033[0m')\n",
    "print('Mean absolute error :', mean_absolute_error(Y_test,y_pred))\n",
    "print('Mean squared error :', mean_squared_error(Y_test,y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(Y_test,y_pred)))\n",
    "print('\\n')\n",
    "print('\\033[1m'+' R2 Score of Final Model :'+'\\033[0m')\n",
    "print(r2_score(Y_test,y_pred)) \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "y_pred=Final_mod.predict(X_test)\n",
    "sns.swarmplot(Y_test.round(2), y_pred)\n",
    "print('\\033[1m'+' True Values Vs Predicted Value plot :' +'\\033[0m')\n",
    "plt.xlabel('True Values' , fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Regression Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(Final_mod,'Powerplant_regression_Final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
