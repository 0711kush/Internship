{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. This task will be done in following steps:\n",
    "\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\bhati\\OneDrive\\Desktop\\chromedriver\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening naukri.com\n",
    "url ='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sending Input as “Data Analyst” in “Skill,Designations,Companies” field and  “Bangalore” in “enter the location” field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[6]/div/div/div[1]/div/div/div[1]/input').send_keys('Data Analyst')\n",
    "driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[6]/div/div/div[5]/div/div/div/input').send_keys('Bangalore')\n",
    "driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[6]/div/div/div[6]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists to store job title, job location, company name, experiecne require\n",
    "job_title =[]\n",
    "job_location=[]\n",
    "company =[]\n",
    "experienced=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scraping job title, job location, company name, experiecne require from Naukri website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=driver.find_elements(By.XPATH,'//div[@class=\"info fleft\"]/a')[:10]\n",
    "company_title=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')[:10]\n",
    "company_location= driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')[:10]\n",
    "experience_require=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### appending scrap data in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles:\n",
    "    job_title.append(i.text)\n",
    "for i in company_title:\n",
    "    company.append(i.text)\n",
    "for i in company_location:\n",
    "    job_location.append(i.text)\n",
    "for i in experience_require:\n",
    "    experience=i.text\n",
    "    experienced.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_analyst=pd.DataFrame({})\n",
    "Data_analyst['JOB TITLE']=job_title\n",
    "Data_analyst['COMPANY']=company\n",
    "Data_analyst['JOB LOCATION']=job_location\n",
    "Data_analyst['Experience Require']=experienced\n",
    "print('/033[1m'+'Top 10 Data Analyst Job at Bangrole :'+'/033[0m')\n",
    "Data_analyst[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location,company_name, full job-description. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- 1. All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sending Input as “Data Scientist” in “Skill,Designations,Companies” field and  “Bangalore” in “enter the location” field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[6]/div/div/div[1]/div/div/div[1]/input').send_keys('Data Scientist')\n",
    "driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[6]/div/div/div[5]/div/div/div/input').send_keys('Bangalore')\n",
    "driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[6]/div/div/div[6]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty lists\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "job_description=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the list of url of first 10 data scientist jobs\n",
    "\n",
    "URL=[]\n",
    "job_tags=driver.find_elements(By.XPATH,\"//a[@class='title fw500 ellipsis']\")[0:10]\n",
    "for i in job_tags:\n",
    "    URL.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in URL:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        job_title.append((driver.find_element(By.XPATH,'//*[@id=\"root\"]/main/div[2]/div[2]/section[1]/div[1]/div[1]/header/h1')).text)\n",
    "        job_location.append((driver.find_element(By.XPATH,'//*[@id=\"root\"]/main/div[2]/div[2]/section[1]/div[1]/div[2]/div[3]/span')).text)\n",
    "        company_name.append((driver.find_element(By.XPATH,'//*[@id=\"root\"]/main/div[2]/div[2]/section[1]/div[1]/div[1]/div/a')).text)\n",
    "        job_description.append((driver.find_element(By.XPATH,'//*[@id=\"root\"]/main/div[2]/div[2]/section[2]')).text.replace(\"\\n\",\"\"))\n",
    "    except NoSuchElementException:\n",
    "        job_title.append(\"Nan\")\n",
    "        job_location.append(\"Nan\")\n",
    "        company_name.append(\"Nan\")\n",
    "        job_description.append(\"Nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scientist=pd.DataFrame({})\n",
    "Data_scientist['job_title']=job_title\n",
    "Data_scientist['job_location']=job_location\n",
    "Data_scientist['company_name']=company_name\n",
    "Data_scientist['job_description']=job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m'+'Top 10 Data Analyst Job at Bangrole :'+'\\033[0m')\n",
    "Data_scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: In this question you have to scrape data using the filters available on thewebpage as shown below: You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company_name, experience_required. The location filter to be used is “Delhi/NCR” . The salary filter to be used is “3-6” lakhs.\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[6]/div/div/div[1]/div/div/div[1]/input').send_keys('Data Scientist')\n",
    "driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[6]/div/div/div[6]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticking on Delhi/NCR \n",
    "driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[4]/div/section[1]/div[2]/div[2]/div[2]/div[2]/label/p/span[1]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticking on 3-6 lakhs Salary\n",
    "driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[4]/div/section[1]/div[2]/div[3]/div[2]/div[1]/label/i').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists to store job title, job location, company name, experiecne require\n",
    "job_title =[]\n",
    "job_location=[]\n",
    "company =[]\n",
    "experienced=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scraping job title, job location, company name, experiecne require from Naukri website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')[:10]\n",
    "company_title=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')[:10]\n",
    "company_location= driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')[:10]\n",
    "experience_require=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles:\n",
    "    job_title.append(i.text)\n",
    "for i in company_title:\n",
    "    company.append(i.text)\n",
    "for i in company_location:\n",
    "    job_location.append(i.text)\n",
    "for i in experience_require:\n",
    "    experience=i.text\n",
    "    experienced.append(experience)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scientist=pd.DataFrame({})\n",
    "Data_scientist['JOB TITLE']=job_title\n",
    "Data_scientist['COMPANY']=company\n",
    "Data_scientist['JOB LOCATION']=job_location\n",
    "Data_scientist['Experience Require']=experienced\n",
    "print('/033[1m'+'Top 10 Data Analyst Job at Bangrole :'+'/033[0m')\n",
    "Data_scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %\n",
    "\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it.\n",
    "\n",
    "5. Now scrape data from this page as usual\n",
    "\n",
    "6. repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.flipkart.com/'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sending Input as 'Sunglasses' in search field field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input').send_keys('sunglasses')\n",
    "driver.find_element(By.XPATH,'//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty lists\n",
    "name=[]\n",
    "des=[]\n",
    "price=[]\n",
    "dis=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping URl of 100 Sunglasses\n",
    "URL=[]\n",
    "for i in range(0,3):\n",
    "    S_g_url=driver.find_elements(By.XPATH,\"//nav[@class='yFHi8N']//a\")\n",
    "    for i in S_g_url:\n",
    "        URL.append(i.get_attribute('href'))\n",
    "    # Clicking on NEXT button at end of page\n",
    "    driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]').click()\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in URL[:3]:\n",
    "    driver.get(i)\n",
    "    title=driver.find_elements(By.CLASS_NAME,\"_2WkVRV\")\n",
    "    for i in title:\n",
    "         name.append(i.text)\n",
    "            \n",
    "    desc=driver.find_elements(By.CLASS_NAME,'IRpwTa')\n",
    "    for i in desc:\n",
    "        des.append(i.text)\n",
    "        \n",
    "    amount=driver.find_elements(By.CLASS_NAME,'_30jeq3')\n",
    "    for i in amount:\n",
    "        price.append(i.text)\n",
    "        \n",
    "    less=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "    for i in less:\n",
    "        dis.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 Sunglasses from Flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sunglasses=pd.DataFrame({})\n",
    "Sunglasses['Brand']=name[:100]\n",
    "Sunglasses['Product_Description']=des[:100]\n",
    "Sunglasses['Price']=price[:100]\n",
    "Sunglasses['Discount']=dis[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sunglasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. \n",
    "You have to go the link: https://www.flipkart.com/apple-iphone-11-white-128-gb/p/itme32df47ea6742?pid=MOBFWQ6B7KKRXDDS&lid=LSTMOBFWQ6B7KKRXDDSUPVRTR&marketplace=FLIPKART&q=iphone+11&store=tyy%2F4io&srno=s_1_1&otracker=AS_QueryStore_OrganicAutoSuggest_1_6_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_1_6_na_na_na&fm=organic&iid=8b9325df-98b3-498d-89de-d1a10f8f8a58.MOBFWQ6B7KKRXDDS.SEARCH&ppt=hp&ppn=homepage&ssid=o0ppn9bno00000001664723071893&qH=f6cdfdaa9f3c23f3\n",
    "\n",
    "When you will open the above link you will reach to the below shown webpage. As shown in the above page you have to scrape the tick marked attributes.\n",
    "These are\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.flipkart.com/apple-iphone-11-white-128-gb/p/itme32df47ea6742?pid=MOBFWQ6B7KKRXDDS&lid=LSTMOBFWQ6B7KKRXDDSUPVRTR&marketplace=FLIPKART&q=iphone+11&store=tyy%2F4io&srno=s_1_1&otracker=AS_QueryStore_OrganicAutoSuggest_1_6_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_1_6_na_na_na&fm=organic&iid=8b9325df-98b3-498d-89de-d1a10f8f8a58.MOBFWQ6B7KKRXDDS.SEARCH&ppt=hp&ppn=homepage&ssid=o0ppn9bno00000001664723071893&qH=f6cdfdaa9f3c23f3'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/button').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "page_url = [] # a list containg url's of every page\n",
    "url = driver.find_elements(By.XPATH,\"//nav[@class='yFHi8N']//a\")\n",
    "for i in url:\n",
    "    page_url.append(i.get_attribute('href'))\n",
    "\n",
    "len(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating=[]\n",
    "review=[]\n",
    "fullre=[]\n",
    "for i in page_url:\n",
    "    driver.get(i)\n",
    "    rate=driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rate:\n",
    "        rating.append(i.text)\n",
    "    rev=driver.find_elements_by_class_name(\"_2-N8zT\")\n",
    "    for i in rev:\n",
    "        review.append(i.text)\n",
    "    fr=driver.find_elements_by_class_name(\"t-ZTKy\")\n",
    "    for i in fr:\n",
    "        fullre.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m'+'Iphone 11 Reviews from Flipkart :'+'\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Review_Iphone11= pd.DataFrame({})\n",
    "Review_Iphone11['Rating'] = Rating[:100]\n",
    "Review_Iphone11['Review Summary'] = Review_summary[:100] \n",
    "Review_Iphone11['Full Review'] = Full_review[:100]\n",
    "Review_Iphone11.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "\n",
    "As shown in the below image, you have to scrape the tick marked attributes.Also note that all the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input').send_keys('sneakers')\n",
    "driver.find_element(By.XPATH,'//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty lists\n",
    "Brand=[]\n",
    "Product_Description=[]\n",
    "Price=[]\n",
    "Discount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the list of url of first 100 sneakers\n",
    "URL=[]\n",
    "for i in range(0,4):\n",
    "    sneakers=driver.find_elements(By.XPATH,\"//a[@class='IRpwTa']\")\n",
    "    for i in sneakers:\n",
    "        URL.append(i.get_attribute('href'))\n",
    "    driver.find_element(By.XPATH,'//a[@class=\"ge-49M\"]').click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_url=URL[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a loop to extract all the required information from the list_of_url\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(list_of_url):\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH,\"//span[@class='G6XhRU']\").text\n",
    "        Brand.append(brand)\n",
    "        description=driver.find_element(By.XPATH,\"//span[@class='B_NuCI']\").text\n",
    "        Product_Description.append(description)\n",
    "        price=driver.find_element(By.XPATH,\"//div[@class='_30jeq3 _16Jk6d']\").text\n",
    "        Price.append(price)\n",
    "        discount=driver.find_element(By.XPATH,\"//div[@class='_3Ay6Sb _31Dcoz pZkvcx']/span\").text\n",
    "        Discount.append(discount)\n",
    "    except NoSuchElementException:\n",
    "        Brand.append(\"Null\")\n",
    "        Product_Description.append(\"Null\")\n",
    "        Price.append(\"Null\")\n",
    "        Discount.append('Null')\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Brand),len(Product_Description),len(Price),len(Discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sneakers=pd.DataFrame({})\n",
    "Sneakers['Brand']=Brand[:100]\n",
    "Sneakers['Product_Description']=Product_Description[:100]\n",
    "Sneakers['Price']=Price[:100]\n",
    "Sneakers['Discount']=Discount[:100]\n",
    "Sneakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon.Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "    \n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price\n",
    "\n",
    "As shown in the below image as the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening amazon.com\n",
    "url ='https://www.amazon.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'//*[@id=\"twotabsearchtextbox\"]').send_keys('laptop')\n",
    "driver.find_element(By.XPATH,'//*[@id=\"nav-search-submit-button\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'//*[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]/span/a/div').click()\n",
    "driver.find_element(By.XPATH,'//*[@id=\"p_n_feature_thirteen_browse-bin/16757432031\"]/span').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "rating=[]\n",
    "price=[]\n",
    "tt=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in tt:\n",
    "    title.append(i.text)\n",
    "    \n",
    "title=title[0:10]\n",
    "\n",
    "rat=driver.find_elements(By.CLASS_NAME,'a-icon-alt')\n",
    "for i in rat:\n",
    "    rating.append(i.get_attribute('textContent'))\n",
    "    \n",
    "rating=rating[0:10]\n",
    "\n",
    "pr=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in pr:\n",
    "    price.append(i.text)\n",
    "    \n",
    "price=price[0:10]\n",
    "\n",
    "que8=pd.DataFrame({'Title':title,'Rating':rating,'Price':price})\n",
    "que8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "driver = webdriver.Chrome()  # Replace with the path to your web driver executable\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "\n",
    "# Step 2: Click on TopQuotes\n",
    "top_quotes_link = driver.find_element(By.LINK_TEXT, \"Top Quotes\")\n",
    "top_quotes_link.click()\n",
    "\n",
    "# Step 3: Scrape Quote, Author, and Type of Quotes\n",
    "quotes = driver.find_elements(By.XPATH, \"//div[@class='wrap-blockquote']\")\n",
    "\n",
    "for quote in quotes:\n",
    "    quote_text = quote.find_element(By.XPATH, \".//a[@class='title']\")\n",
    "    author = quote.find_element(By.XPATH, \".//span[@class='author']\")\n",
    "    quote_type = quote.find_element(By.XPATH, \".//div[@class='qll-bg']\")\n",
    "\n",
    "    print(\"Quote:\", quote_text.text)\n",
    "    print(\"Author:\", author.text)\n",
    "    print(\"Type of Quote:\", quote_type.text)\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make theDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 2: Click on the GK option\n",
    "gk_link = soup.find(\"a\", text=\"GK\")\n",
    "gk_url = gk_link[\"href\"]\n",
    "\n",
    "# Step 3: Click on the List of all Prime Ministers of India\n",
    "gk_response = requests.get(gk_url)\n",
    "gk_soup = BeautifulSoup(gk_response.text, \"html.parser\")\n",
    "\n",
    "pm_link = gk_soup.find(\"a\", text=\"List of all Prime Ministers of India\")\n",
    "pm_url = pm_link[\"href\"]\n",
    "\n",
    "# Step 4: Scrape the data and create the DataFrame\n",
    "pm_response = requests.get(pm_url)\n",
    "pm_soup = BeautifulSoup(pm_response.text, \"html.parser\")\n",
    "\n",
    "table = pm_soup.find(\"table\", class_=\"table4\")\n",
    "\n",
    "data = []\n",
    "headers = []\n",
    "for index, row in enumerate(table.find_all(\"tr\")):\n",
    "    if index == 0:\n",
    "        headers = [header.text for header in row.find_all(\"th\")]\n",
    "    else:\n",
    "        row_data = [cell.text for cell in row.find_all(\"td\")]\n",
    "        data.append(row_data)\n",
    "\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.Car name and Price) from https://www.motor1.com/\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.motor1.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 2: Type in the search bar '50 most expensive cars'\n",
    "search_input = soup.find(\"input\", attrs={\"id\": \"search\"})\n",
    "search_input['value'] = '50 most expensive cars'\n",
    "\n",
    "# Step 3: Click on '50 most expensive cars in the world'\n",
    "search_form = soup.find(\"form\", attrs={\"class\": \"search-form\"})\n",
    "submit_button = search_form.find(\"button\", attrs={\"type\": \"submit\"})\n",
    "search_url = url + search_form['action']\n",
    "search_response = requests.get(search_url, params={\"q\": '50 most expensive cars'})\n",
    "search_soup = BeautifulSoup(search_response.text, \"html.parser\")\n",
    "\n",
    "result_link = search_soup.find(\"a\", text=\"50 Most Expensive Cars in the World\")\n",
    "\n",
    "# Step 4: Scrape the data and create the DataFrame\n",
    "result_url = result_link[\"href\"]\n",
    "result_response = requests.get(result_url)\n",
    "result_soup = BeautifulSoup(result_response.text, \"html.parser\")\n",
    "\n",
    "cars = result_soup.find_all(\"article\", class_=\"spec-item\")\n",
    "\n",
    "data = []\n",
    "for car in cars:\n",
    "    car_name = car.find(\"h3\").text.strip()\n",
    "    car_price = car.find(\"span\", class_=\"spec-price\").text.strip()\n",
    "\n",
    "    data.append([car_name, car_price])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Car Name\", \"Price\"])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
